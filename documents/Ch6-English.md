### CHAPTER 6 Selective predators and responsive prey



*[M]ost carnivores do not confine themselves rigidly to one kind of prey; so that when their food of the moment becomes scarcer than a certain amount, the enemy no longer finds it worth while to purse this particular one and turns its attention to some other species instead.*

*Charles Elton, 1927. p. 122*

*The food of every carnivorous animal lies therefore between certain size limits, which depend partly on its own size and partly on other factors. There is an optimum size of food which is the one usually eaten and the limits actually possible are not usually realized in practice.*

*Charles Elton, 1927, p. 60*



The previous chapter focused on the simplest possible predator–prey interaction—one predator species feeding on a single species of prey. Although this is a logical place to start, such extreme specialization is rare in nature (Williamson 1972; Schoener 1989). In reality, most predators feed on a variety of prey types. The fact that a predator may consume different prey species has important consequences, both from the perspective of the predator and from that of its prey. From the predator’s point of view, there are several interesting questions. Are predators selective? Do they prefer some prey types while ignoring others? Does their preference change with prey density or prey behavior? How should we expect a predator’s diet to change under different ecological conditions? From the prey’s vantage point, selective predation has important consequences for mortality rates, thereby affecting prey behaviors, adaptations, and population dynamics, as well as the coexistence and diversity of prey species. This chapter explores some of these issues, starting from the predator’s point of view and asking why predators prefer some prey types over others.



#### Predator preference

A predator’s preference for a given prey type can be defined as the difference between the proportion of that prey type in a predator’s diet compared with the proportion of that prey type present in the environment. Thus, preference can be positive (a prey is selected for) or negative (a prey is selected against). It is useful to express predator preference in a way that allows comparisons between prey types, or between predators under different environmental conditions. Therefore, ecologists have developed a number of indices to measure preference (Manly 1985). The most widely used of these is the index first proposed by Manly (1974) and expanded on by Chesson (1978, 1983). The Manly– Chesson index calculates predator preference for prey type i as

ai = (di/Ni) / ∑k-j=1(dj/Nj) [Eqn 6.1]

where i = 1, 2, …, k, and where k is the number of prey categories, di is the number (or proportion) of prey of type i in the predator’s diet, and Ni is the number (or proportion) of prey of type i in the environment. The index α i ranges from 0 to 1. Prey types that are consumed in proportion to their abundance in the environment (i.e., no preference) have α ı = 1/k; αi > 1/k indicates positive preference for a prey type, and αi < 1/k indicates negative preference for a prey type.

Why should predators prefer to eat some prey types and not others? We can approach this question by breaking down the act of predation into its component parts (Figure 6.1). Three main factors influence predator preference:

1. The probability that a prey item will be encountered (e.g., its “visibility” in the broadest sense).

2. The probability that an encountered prey item will be attacked (the predator’s choice to go after a prey item or not).

3. The probability that an attacked prey item will be successfully captured and eaten.

Figure 6.1 Principal elements of the predation process (center column) and factors affecting the process. The right-hand column lists the three main factors that determine the probability that a prey of type i will be consumed. Listed to the left are attributes of predators and prey that influence these probabilities. After Mittelbach (2002). If a prey item is cryptic and hard to see, then we would expect it to be relatively rare in the predator’s diet and to be selected against simply because it is “encountered” at a low rate. In general, factors that increase a prey’s encounter rate or increase a predator’s capture success should have a positive effect on predator preference.

What about active choice by the predator (see Figure 6.1)? Why do predators choose to go after some prey types and ignore others? Since the mid- 1960s, ecologists have sought a general answer to this question by recognizing that natural selection should favor individuals that harvest food efficiently (Krebs 1978). Therefore, just as a functional morphologist might ask how natural selection has modified the jaw structure of an organism to maximize its crushing force, we can ask what set of foraging decisions should result in the most efficient energy capture. This approach to understanding diet choice has become known as optimal foraging theory (OFT).

##### Optimal foraging theory leads to a model of predator diet choice

Consider a predator actively moving through its environment and encountering potential prey items (e.g., a bird hopping from branch to branch searching for insects). If prey are randomly distributed and encountered sequentially, and if the predator’s decision to pursue and eat a prey item results in time spent handling prey that is unavailable for searching, then we can model a predator’s rate of energy gain as follows.

Let E be the energy gained during a feeding period of length T, which is composed of time spent searching for prey (Ts) and time spent handling prey items (Th). If the predator expends the same amount of energy during searching and during handling, then the predator’s rate of energy gain (E/T) is 

rate of energy gain E / T = E / (Th + Ts)

Now let there be k prey types in the environment, with each prey type i having the following characteristics:

λı = number of prey of type i encountered per unit of search time (this means λiTs individual prey will be encountered for a given amount of search time)

pi = probability that the predator will pursue and capture a prey of type i after it is encountered. (this means λ iTsPi individual prey will be pursued for a given amount of search time)

Ei = energy gain from consuming one individual of prey type i

hi = handling time for handling one individual of prey type i

The variable pi is under the control of the predator and represents the predator’s choice to attack a prey type or not. Given these properties, it follows that

E = ∑ik λi iT p s i ,

and

Th = ∑k λi iT p s i ,

then

[Eqn 6.2]  

or

-

Equation 6.2 may look familiar—it is of the same general form as Holling’s equation for a type II predator functional response (Equation 5.2), except that Equation 6.2 includes multiple prey types, the energy gained from each prey of type i (Ei), and a parameter pi representing the probability that a predator will pursue and capture a prey of type i.

Equation 6.2 is the standard optimal diet model. It was derived by several ecologists during the early 1970s (e.g., Schoener 1971; Emlen 1973; Maynard Smith 1974; Pulliam 1974; Werner and Hall 1974; Charnov 1976a) as they wrestled with the question of what rules of prey choice would yield the greatest energy gain per unit time spent foraging (i.e., the optimal diet). The model makes two general predictions and two more specific predictions about optimal diet choice. Its general predictions are that:

• foragers should prefer the most profitable prey (those that yield the most energy per unit handling time, Ei/hi);

• an efficient forager should broaden its diet to include more low-value prey as the abundance of higher-value prey decreases (Stephens and Krebs 1986; Sih and Christensen 2001). 

There is considerable evidence that predators prefer to feed on the most profitable prey when prey are abundant (Figure 6.2) and that predator diets narrow as the abundance of more profitable prey increases (Figure 6.3). Thus, the general predictions of optimal foraging theory are well supported (Sih and Christensen 2001).  

Figure 6.2 Predators prefer to eat more profitable prey. (A) In a laboratory study of crabs feeding on mussels (Elner and Hughes 1978), prey profitability (energy gain versus handling time) peaked at intermediate mussel sizes (solid curved line), and crabs preferred to eat mussels of intermediate sizes when offered a choice (histograms). (B) The preferred size of prey consumed in the wild by a small fish, the 15-spined stickleback (Spinachia spinachia) plotted against the stickleback’s optimal prey size, based on prey mass/handling time (see Kislaliogu and Gibson 1976). The number next to each data point denotes stickleback length; both optimal and preferred prey sizes increase with the predator’s size. After Krebs (1978).

The standard optimal diet model also makes two quite specific predictions about the nature of prey selection and how it should change with prey abundance. The first of these predictions is that prey types are either always eaten upon encounter (pi = 1) or never eaten upon encounter (pi = 0), which is known as the zero-one rule (Stephens and Krebs 1986). The second is that the inclusion of a prey type in the diet depends only on its profitability and on the characteristics of prey types of higher profitability (i.e., the inclusion of a prey type does not depend on its own encounter rate; Stephens and Krebs 1986). To understand where these predictions come from, we can use a simple algorithm to determine the predator’s optimal diet. Following Charnov (1976a), we rank prey items from most profitable (highest Ei/hi) to least profitable (Figure 6.4). We then add prey types to the diet in rank order (starting with the most profitable prey) until the rate of energy gain (E/T ) is maximized (written as E*/T *). The optimal diet is the set of all prey types (i) whose value satisfies

Ei/hi ≥ E∗/T∗ [Eqn 6.3]

In words, Equation 6.3 says that a predator should choose to pursue a prey item only if the rate of energy it gains from consuming that prey item (Ei/hi) is greater than or equal to the rate of energy it gains by eating all prey items of higher rank. Note that the decision to include a prey type in the diet depends on the prey’s profitability, but not on its rate of encounter, and that a prey item is either always eaten or always ignored—the two specific predictions of the optimal diet model. How well do foragers in nature match these two predictions?

Figure 6.3 Two classic experiments tested the predictions of the standard optimal diet model. (A) Small bluegill sunfish (Lepomis macrochirus) in wading pools were offered a mixture of three size classes of zooplankton (Daphnia), and their diet choice was observed at low, medium, and high prey densities (data from Werner and Hall 1974). Ranking of prey profitability (Daphnia biomass/handling time) was large prey > medium prey > small prey. Histograms show the ratios of encounter rates with each Daphnia size class at the three prey densities, along with the predicted optimal diets and the observed bluegill diets. (B) For this experiment, Krebs et al. (1977) trained great tits (Parus major) to feed from a moving conveyor belt, onto which the researchers placed different-sized mealworms at different densities. The conveyor belt was then manipulated to adjust the rate at which birds encountered prey. Histograms show the proportion of large and small mealworm prey encountered; predicted to comprise the optimal diet; and observed to be eaten by the birds. After Krebs (1978).

Looking again at Figure 6.3, which summarizes the results of two classic tests of the optimal diet model, it can be seen that the foragers exhibited close, but not perfect, correspondence to the optimal diet model’s predictions. In both studies, less profitable prey were dropped from the diet as the abundance of more profitable prey increased. In the case of great tits feeding on different-sized mealworms (see Figure 6.3B), increasing a bird’s encounter rate with prey that were outside the optimal diet had only a minor impact on the bird’s observed diet. However, in both studies, foragers included some “suboptimal” prey in their diets.

Only a few studies have attempted to test the optimal diet model in the field (e.g., Meire and Ervynck 1986; Richardson and Verbeek 1986; Cayford and Goss-Custard 1990) because of the difficulty of estimating rates of prey encounter. One of us (Mittelbach 1981) attempted such a test with bluegills in a small Michigan lake as part of their PhD research and found that bluegills preferred to eat large prey, and that their diets were similar to those predicted by the standard optimal diet model (Figure 6.5). Meire and Ervynck (1986) found a similar result for oystercatchers feeding on mussels on tidal flats in the Netherlands.

The examples given above—and nearly all other tests of the optimal diet model—have shown that feeding efficiency and maximization of energy gain play important roles in determining predator diet choice. In their review of optimal foraging theory, Sih and Christensen (2001) concluded that 87% of studies (31 of 35) that included quantitative tests supported the predictions of the optimal diet model. Thus, optimal foraging theory has significantly increased our understanding of why some prey are selected and some ignored. However, in every study reviewed by Sih and Christensen, predators included some prey types in their diets that were not in the predicted optimal set (see also Pyke 1984; Stephens and Krebs 1986). Does this mean that optimal foraging theory is fundamentally flawed? Some would say yes, arguing that animals cannot possibility make the “optimal” choice (Gray 1986; Pierce and Ollason 1987). However, we know that some of the assumptions of the standard optimal diet model are unlikely to be true (i.e., the assumption of perfect predator knowledge of prey quality and prey density). Therefore, we would not expect the theory to make totally accurate predictions. What we need to know is how “imperfect knowledge” affects prey selection and what more realistic models of prey selection imply for the population dynamics of predator and prey. In particular, it seems sensible that if an organism has imperfect knowledge about handling times and energy of prey, occasional non-optimal sampling would occur to increase information, even if it slightly decreases efficiencies below the maximal. 

Figure 6.4 Charnov’s graphic illustration of how to determine the predator’s optimal diet from the standard optimal diet model (Eqn 6.2). Prey types are first ranked by their profitability (E i/hi); then the net rate of energy gain by the predator is calculated by adding prey types to the diet in rank order. The optimal diet is that set of prey types that maximizes the predator’s total net rate of energy gain (En/T). This optimal prey set contains all prey types (red dots) whose profitabilities rank above the prey type at which En/T first becomes greater than Ei /hi, denoted by the dashed line. After Charnov (1976a).

We share Rosenzweig’s (1995, p. xvii) puzzlement over the relative neglect of optimal foraging studies today:

*Here is a fundamental research program (Mitchell and Valone 1990) originally suggested by no less than Charles Elton (1927). It is linking up behavior, natural selection and ecology. Yet, its critics, taking no time to understand its mathematical substance, and confusing ‘optimal’ with ‘perfect,’ consider it dead because. . .Well, I really don’t know why they think it is dead. Students of optimality know that ‘constraints’ prevent perfect outcomes in the real world. But, ‘constraints’ are not foreign to optimality theory. They form one of its central features. Optimalists know that, work with it, and have produced some. . . exciting ecology . . . by sticking to their rusty old guns.*

There seems to be something of a recent uptick in publications on optimal foraging, suggesting that this discipline may have found new legs (e.g., Stephens et al. 2007; Calcagno et al. 2014; Garay et al. 2015). A revival of this important field linking behavior and ecology would be a welcome event.

The early developers of optimal foraging theory (MacArthur and Pianka 1966, along with Emlen 1966) envisioned that it could provide a tool to predict consumer diets, thereby providing a mechanism to better understand consumer–resource interactions and community dynamics. Unfortunately, this potential application of OFT to community ecology got lost in the rush to develop and test optimal foraging models, although a few studies (e.g., Gleeson and Wilson 1986; Fryxell and Lundberg 1994; Ma et al. 2003) showed how optimal foraging models could be used to explore predator and prey dynamics. A more recent and exciting development is the application of OFT to predict the potential feeding links in size-based food webs (Beckerman et al. 2006; Petchey et al. 2008; Thierry et al. 2011). By modeling prey handling time as an increasing function of the ratio of prey size to predator size, and by assuming that prey energy value is a positive function of prey size, Petchey and colleagues (2008) were able to correctly predict up to 65% of the predator–prey links in four real-world food webs, based on the criteria that predators prefer to eat the most energetically rewarding prey. This application of OFT to predict who eats whom in nature comes close to achieving the goals envisioned by MacArthur and Pianka (1966) when they first introduced OFT.

The application of optimal foraging theory to food webs will be considered in more detail in Chapter 11. In the meantime, we should keep in mind two important insights gained from OFT, as we explore the interactions between predators and prey:

1. All else being equal, predators should prefer to eat the most profitable prey (those that provide the highest energy gain per handling time).

2. As the overall density of prey in the environment decreases, predator diets should broaden to include more prey types. Conversely, as the overall density of prey in the environment increases, predator diets should narrow (predators should become more specialized).

Figure 6.5 A field test of optimal diet predictions. The top row graphs the size and frequency distributions of prey found in samples from two habitats in a small Michigan lake (vegetation and open water); the predicted diets for bluegill sunfish (with an average body length of 125 mm) are in the center graphs; and the actual diets of bluegills feeding in the two habitats on the dates indicated are in the bottom row. The data support the bluegill preference for larger prey, as predicted by the optimal diet model. Note that in the vegetation habitat, the fish were able to find some larger-sized prey items than were sampled by the researcher. After Mittelbach (1981).  

Figure 6.6 Selective predation affects the coexistence and diversity of competing prey. This classic study by Lubchenco (1978) illustrates how selective predation by the snail Littorina littorea may affect the diversity and species richness of algae growing along the rocky New England coast. Each point represents a single study site. (A) In tide pools, where the snails grazed preferentially on the competitively dominant algal species, both algal diversity (top) and algal species richness (bottom) peaked at intermediate snail densities. (B) On emergent substrates, where the snails grazed preferentially on the inferior algal competitors, increasing snail density had a negative effect on species richness and diversity. After Lubchenco (1978).  

##### Consequences of selective predation for species coexistence

The fact that predators prefer to eat some prey and not others has important consequences for the structure of prey communities. For example, in a classic study of intertidal communities along the New England coast, Lubchenco (1978) showed that selective predation may increase prey diversity when predators feed preferentially on prey species that are competitively dominant, but that it decreases diversity when predation falls more heavily on inferior competitors (Figure 6.6). Leibold et al. (2017) provided a more recent example, showing that grazing by zooplankton lead to a doubling in phytoplankton (algae) species richness in experimental mesocosms. The additional phytoplankton species in grazed mesocosms were larger and, therefore, more grazer-resistant. Another example of how selective predation may influence the coexistence of prey species is apparent competition. Holt (1977) and Holt et al. (1994) showed that shared predation among non-competing prey species may produce mutually negative interactions between those species, and that the consequences for biodiversity of this “apparent competition” depend (in part) on whether predator preferences change with changing prey densities. These and other consequences of selective predation and shared predation will be considered in more detail in Chapters 7, 10, and 14, after we have examined interspecific competition.



#### Predator movement and habitat choice

The thinking of optimal foraging theory can be applied not only to understand diet choice, but also to understand how predators make decisions regarding where to feed (habitat choice) and how to move about in a landscape where prey are patchily distributed. Fretwell and Lucas (1970) provided an early model for habitat choice, based on the idea that (all else being equal) animals should select habitats that maximize their fitness (here, fitness is equated with maximizing feeding rate). In the simplest version of their model, termed the ideal free distribution (IFD), food is supplied to a foraging habitat or patch at a fixed rate, where it is consumed immediately. If predators are “free” to select among patches, have perfect or “ideal” knowledge of the rates of resource supply, and do not differ in their competitive abilities, then we expect the number of predators in a patch to be proportional to the total food input to the patch. This is called the “inputmatching rule.” A number of experimental studies show support for predictions of the IFD, including the classic study by Power (1984) of armored catfish (Loricaridae) feeding on periphyton (attached algae) in a stream in Panama. In Power’s study, algal productivity differed among stream pools due to differences in canopy shading and the density of armored catfish in a pool was directly proportional to the percentage of open canopy over the stream (i.e., more fish in sunny, more productive pools). Moreover, algal abundance (standing crop), fish growth rates, and estimates of feeding rates were similar in both sunny and shaded pools, matching expectations of ideal free habitat selection based on consumerresource dynamics (Lessels 1995; Oksanen et al. 1995).

Although the input-matching rule is elegant in its simplicity, there are many reasons why we might expect nature to deviate from the assumptions of the IFD. For example, IFD theory assumes that predators pay no cost in choosing habitats, hence, the epithet “free”. However, in nature costs are real. One particularly important “cost” is the risk of injury or death (i.e., more rewarding habitats may also be riskier). The question of optimal habitat choice when organisms face a trade-off between foraging gain and mortality risk will be explored later, when we consider the non-consumptive effects of predators. Resources in a habitat may also be depleted over time, lowering feeding rates and causing predators to decide whether to stay in a particular patch or leave a patch in search of other, more rewarding patches. The question of whether to stay or leave is addressed in another fundamental model of optimal foraging, known as the marginal value theorem (MVT; Charnov 1976b; Stephens and Krebs 1986).

The marginal value theorem predicts how long a forager should stay in a resource patch before leaving in search of another patch. Two quantities influence this decision:

• the current rate of energy gain in a patch;

• the average rate of energy gain for the environment as a whole.

The MVT assumes that the rate of energy gain in a patch decreases over time as the predator depletes the number of prey in the patch, such that the curve of cumulative energy gain in a patch is decelerating (Figure 6.7A). At some point, declining energy gain within a patch dictates that a forager would be better off leaving and going to search for another patch where resource levels are higher. The question is, when is the optimal time to leave (here, we define the optimal strategy as maximizing the forager’s expected rate of energy gain). If all patches are identical, the answer is quite simple and depends on the travel time between patches. By drawing a line from the average travel time between patches (point a) and intersecting the energy gain curve, the slope of this line defines the predator’s expected rate of energy gain (i.e., energy gained in a patch divided by the sum of the time spent feeding in a patch plus the time travelling between patches). A little reflection makes it clear that the slope of this line will be greatest when the line just touches (is tangent to) the energy gain curve (as in Figure 6.7A). Any other intersection leads to a shallower slope and lower rate of energy gain. The point where the line is tangent to the curve defines the optimal time that a predator should stay in a patch before moving to another patch—any other leaving time would result in a lower rate of energy gain.

Figure 6.7B,C shows how changes in the average travel time between patches and changes in average patch quality (resource abundance) affect a predator’s optimal time to leave a patch. As travel time between patches increases, predators should stay longer in a patch before leaving (Figure 6.7B). An increase in average patch quality in the environment will affect a predator’s optimal residence time differently, depending on the form of the predator’s functional response (Calcagno et al. 2014). Optimal leaving times should increase with increased patch quality for predators with a Type II (decelerating) functional response, but should decrease for predators with a Type III (accelerating) functional response (Figure 6.7C). When patches within a habitat differ in their resource abundance, we can no longer find the optimal solution using the graphical method shown in Figure 6.7A. However, the mathematical solution is simple enough; “The predator should leave the patch it is presently in when the marginal capture rate in the patch [i.e., its current rate on energy gain] drops to the average capture rate for the habitat” (Charnov 1976b). If we draw lines whose slopes correspond to the average rate of energy gain across all patches in the habitat, then the point where these lines of equal slope are tangent to the energy gain curves for patches of different quality defines the optimal time to leave a patch. As Figure 6.7D illustrates, the time to leave a patch should be shorter in less-rewarding patches.

Note the parallels between the optimal diet and optimal patch-time models. Both focus on maximizing an average rate (i.e., slope or derivative) in units of energy per time. In each case the decision (to add a prey type or move to a new patch) is taken depending on how the rate gained from the current choice compares to the overall average. Add a prey if its energy/time efficiency is greater than the current diet efficiency. Leave a patch if the energy/time it provides falls below the overall landscape-wide average. These are what economists call marginal rates and they are a recurring theme in optimization problems.

Charnov’s (1976b) optimal patch use model and the MVT have been extensively tested in laboratory and field experiments and as Ydenberg et al. (2007; p. 12) note, “The patch model may in fact be the most successful empirical model in behavioral ecology; its basic predictions have been widely confirmed …”. Moreover, community ecologists have made good use of the insights provided by the MVT, particularly the observation that when a forager leaves a patch it reflects patch quality (e.g., Figure 6.7D). Krebs et al. (1974), Brown (1988), and others showed that the time between a predator’s last prey capture in a patch and when it leaves the patch (its “giving-up time” (GUT)), and the density of prey remaining in patch when the predator leaves (the “giving-up density” (GUD)) are surrogate measures of the marginal capture rate in the patch. Thus, the easily measured quantities of GUT and GUD can be used to estimate the value of a habitat to a predator (Brown and Kotler 2004). Community ecologists have measured forager GUT’s and GUD’s in the laboratory and in the field to address fundamental questions related to habitat quality, interspecific competition and species coexistence (Kotler and Brown 2007). However, by far the most extensive application of these measures to community ecology has been to study the non-lethal effects of predators on their prey or, as some have more colorfully termed it, “the ecology of fear.”  

Figure 6.7 (A) A graphical model illustrating the optimal time for a predator to leave a patch based on Charnov's (1976b) MVT. Time (x-axis) is divided into time spent in a patch searching for (and consuming) prey and time spent traveling between patches. The predator's cumulative energy gain (y-axis) is expected to increase with time in a patch, but at a decreasing rate due to prey depletion. The optimal time to leave a patch can be found by drawing a line from point a (the average travel time between patches) that is tangent to the energy gain curve (solid-curved line). The slope of this line corresponds to the forager's maximum achievable rate of energy gain, being greater than any other line originating from point a, and intersecting the energy gain curve (e.g., the two dashed lines). (B) Graph showing how an increase in average travel time between patches increases the optimal time to remain in a patch. (C) Graph showing how greater patch quality within a habitat leads to an increase in the optimal time to remain in a patch (assuming a Type II functional response). (D) In this figure, there are two types of patches in a habitat; a high-quality patch and a low-quality patch (cumulative energy gain is greater in the high-quality patch than in the low-quality patch). According to the MVT, the optimal time to leave a patch is determined by the average rate of energy gain from all patches in the habitat (slope of solid line). The optimal leaving time for a patch corresponds to the point where a dashed line, whose slope is equal to the average rate of energy gain for the habitat, is tangent to the patch's energy gain curve. As panel (D) shows, predators should forage longer in high-quality patches.  



#### The non-consumptive effects of predators

Prior to 1980, most ecological theory viewed predator– prey interactions from the simple perspective of consumptive effects (i.e., predators kill and eat their prey). However, we now know that this view is incomplete. In recent years, ecologists have amassed a wealth of evidence showing that prey may respond to the threat of predation by changing their behaviors, morphologies, physiologies, and/or life histories. These non-lethal, or non-consumptive effects of predators may act in concert with the direct consumption of prey to influence prey abundance and predator–prey dynamics (Werner and Peacor 2003). Furthermore, as noted in Chapter 5, flexible antipredator traits (e.g., increased vigilance or refuge use in the presence of more predators) can make the predator’s functional response predator-dependent; that is, the presence of more predators causes prey to be less “available,” leading to a reduction in each predator’s feeding rate. In this way, flexible antipredator traits may promote the stability of predator– prey interactions (Abrams 1982).

An excellent example of how inducible defenses may change the form of the predator’s functional response is provided by Hammill et al. (2010), who studied predation on the protozoan Paramecium aurelia by the flatworm Stenostomum virginianum. In the presence of the predator, Paramecium reduced their average swimming speed and increased their body width (Figure 6.8A, B). These inducible defenses had a marked effect on the predator’s attack rate and handling time, causing the predator’s functional response to change from a type II (for undefended prey) to a type III (for defended prey). This change was brought about by low attack rates at low prey densities. Simulations using a Lotka–Volterra predator–prey model parameterized for the observed type II and type III functional responses showed that inducible defenses in the Paramecium population reduced the death rate at low prey densities and increased the stability of the predator–prey interaction (Figure 6.8C, D).

A variety of terms have been used to describe the consumptive and non-consumptive effects of predators on predator–prey dynamics and the interactions between predators and prey at different trophic levels. These terms include “trait-mediated indirect effects” and “density-mediated indirect effects” (Abrams 1995; Abrams et al. 1996); “traitmediated indirect interactions” and “density-mediated indirect interactions” (TMII and DMII; Peacor and Werner 1997); and “trait-mediated interactions” (TMI; Bolker et al. 2003) and “density-mediated interactions” (DMI; Bolnick and Preisser 2005; Preisser et al. 2005). As Abrams (2007) notes, the introduction of these different terms has led to some confusion and ambiguity. Therefore, we will stick to the simpler terms “consumptive effects” and “non-consumptive effects” to describe the different impacts that predators have on their prey. The remainder of this chapter discusses a variety of non-consumptive effects, organizing them into four general categories:

• habitat use and habitat shifts;

• life history evolution;

• activity level;

• morphological changes.

We will then ask how important these non-consumptive effects are relative to the impact of predators consuming their prey. As we shall see, this is a critical question that is difficult to answer. In subsequent chapters on food webs and the indirect effects of interactions (see Chapters 10 and 11), the consequences of both consumptive and non-consumptive effects for multispecies interactions will be considered.

##### Habitat use and habitat shifts

The ostrich may (mythically) stick its head in the sand, but most prey show a more adaptive response when predators threaten—they seek refuge. Many studies have documented changes in habitat use by prey in the presence of predators—more than 70 studies were cited in a review by Lima (1998). These habitat shifts may be short term and have little effect on the prey’s population dynamics (e.g., the mouse that hides in the grass as a hawk flies overhead). However, other types of predator-induced habitat shifts have far-reaching effects on prey dynamics and life histories. For example, small (young) individuals tend to be most vulnerable to predators. As a consequence, predators may restrict vulnerable size (age) classes to protective habitats, leading to a change in the prey’s habitat use and diet (if resources differ among habitats) as it grows. These ontogenetic niche shifts (sensu Werner and Gilliam 1984) may reduce competition between size (age) classes within a species (Werner et al. 1983; Mittelbach and Osenberg 1993), but may increase interspecific competition among species and size (age) classes that share a refuge (Mittelbach and Chesson 1987; Persson 1993; Diehl and Eklöv 1995).

In choosing habitats, organisms often face a trade-off between foraging gain and mortality risk (i.e., habitats with more resources tend to be riskier). When this is the case, the “optimal” habitat choice depends on the relative costs (mortality risks) and benefits (energy gains) available in each habitat (Gilliam 1982; Gilliam and Fraser 1987; Ludwig and Rowe 1990; McNamara and Houston 1994). Moreover, if predation risk varies temporally or spatially, or changes over the life history of an organism, we would expect organisms to respond by changing their habitats. An excellent example is vertical migration by some zooplankton. In the open water of lakes and oceans, small crustaceans (zooplankton) that feed on algae (phytoplankton) show a pronounced daily migration cycle, spending the daylight hours in the dark, cold depths and migrating to warm surface waters at dusk, then returning to the depths at dawn. These diel (day–night) vertical migrations can cover tens of meters in freshwater lakes and hundreds of meters in the open ocean; such movements come at a substantial energetic cost. For years, scientists puzzled over the explanation for these migrations. We now know that, in most cases, vertical migrations are a response to temporal variation in predation risk. During the day, mortality rates from fish predation are high in the warm, well-lit surface waters, whereas mortality rates are substantially lower in the cold, dark depths. At night, predation pressure at the surface drops substantially because most fish are visual feeders. A strong vertical gradient in energy gain and developmental rate exists as well; food (phytoplankton, or algae) is often more abundant near the surface, and (more importantly) the warm surface water shortens zooplankton development times and increases birth rates, leading to higher population growth rates (Stich and Lampert 1981; Lampert 1987). Thus, by migrating vertically, zooplankton may balance the conflicting selection pressures of reducing predation risk, and increasing energy gain and reproductive rate.

Figure 6.8 Induced antipredator defenses can stabilize predator–prey dynamics. Changes in the swimming speed (A) and body width (B) of Paramecium after exposure to chemical cues from a predator, the flatworm (Stenostomum). Means ±1 SE. Simulated prey and predator population densities from a Lotka–Volterra predator–prey model incorporating functional responses derived from the empirical data for undefended prey ((C) type II functional response) and defended prey ((D) type III functional response). Induced defenses against predators stabilized the predicted predator–prey dynamics. After Hammill et al. (2010).  

Pangle et al. (2007) showed that the degree of vertical migration of zooplankton in Lake Michigan and Lake Erie was correlated with the abundance of their invertebrate predator, Bythotrephes longimanus. If we assume that zooplankton are migrating in response to the presence of Bythotrephes, then we can estimate the impact of the non-consumptive effect on population growth rate (due to a reduction in birth rate caused by migration into the cold hypolimnion) separately from the consumptive effect on population growth (due to direct mortality). In their analysis, Pangle et al. (2007) found that the non-lethal effects of the predator on zooplankton population growth were often of similar magnitude to the lethal effects.

Diel vertical migration in zooplankton is one particularly well-documented example of a very general response of prey to spatial or temporal variation in predation risk. There are literally hundreds of other examples that show how prey modify their habitat use in response to the threat of predation (reviewed by Lima 1998). Werner (1986) has extended our thinking about the non-consumptive effects of predators to include an even more dramatic type of habitat shift—metamorphosis (described in the following section). In the examples given previously and those reviewed by Lima, the adaptive nature of habitat choice seems clear. However, it should be noted that few studies have attempted to compare an organism’s observed habitat choices with predictions based on a quantitative theory of optimal habitat selection that incorporates both foraging gain and predation risk (e.g., Gilliam and Fraser 1987; Brown 1998). The challenge here is in measuring the potential benefits (e.g., energy gain) and costs (e.g., risk of mortality) of using different habitats, and expressing these costs and benefits in a common currency that can be used to estimate habitat quality. One solution to this problem is to let the organisms do it for us.

Choice experiments provide a means for getting animals to reveal the foraging costs of predation risk (Brown 1988; Brown and Kotler 2004). In such experiments, either energy reward or predation risk is continuously varied among habitats until the forager reveals a point of indifference in their habitat use. For example, an early study by Abrahams and Dill (1989) offered guppies (Poecilia reticulata) a choice between two patches (two sides of an aquarium) that differed in foraging gain (rate of resource supply) and predation risk (presence or absence of a predator). Not surprisingly, more guppies choose the safe half of the aquarium then the risky half, but by varying either the rate of resource supply or the risk of predation, Abrahams and Dill (1989) were able to measure how much higher a guppy’s feeding rate had to be in order for it to accept a greater risk of predation. Behavioral ecologists have used the fact that animals are willing to accept higher predation risk for higher foraging gains to understand the nature of habitat selection in the field.

Recall from the discussion of Charnov’s MVT that the optimal time for a forager to leave a patch is when its current rate of energy gain in the patch equals the average rate of energy gain for all patches in the habitat. Predation risk can be added to the theory of optimal patch use (e.g., Brown 1992; Houston et al. 1993), with the general result that foragers should leave patches sooner (leave at a higher current rate of energy gain) in risky habitats than in safe habitats. This prediction has been tested in the field using a simplified measure of a forager’s feeding rate when it leaves a patch, which can be approximated by the amount of resources remaining in the patch at the time it leaves. This resource density, termed the “giving-up density” or GUD, should be positively correlated with the forager’s rate of energy gain. Numerous studies with small mammals and birds have confirmed that patches near cover have lower GUD’s than patches away from cover and that GUD’s increase as the distance of a patch from a burrow or other refuge increases (reviewed in Brown and Kotler 2004; Kotler and Brown 2007). Thus, foragers perceive “open” patches and patches farther from a refuge as risker and, therefore, demand a higher foraging return for using these richer patches. Researchers have extended these findings to use measures of GUD’s in a variety of habitats to characterize the “landscape of fear” perceived by a forager (Van Der Merwe and Brown 2008; Laundre et al. 2017).

##### Life history evolution

Life history theory predicts that organisms should undergo metamorphosis when the fitness that can be achieved in the larval habitat (e.g., by a tadpole in fresh water) drops below the fitness that could be achieved by shifting to the adult habitat (e.g., by an adult frog in terrestrial habitat; Werner 1986). In the simple case of a stable population under no time constraints, Gilliam (1982) showed that the optimal size to switch from the larval to the adult habitat occurs when μ/g in the larval habitat rises above μ/g in the adult habitat, where μ is a species’ size-specific mortality rate and g is its size-specific growth rate (Figure 6.9). Intuitively, minimizing μ/g at each size permits growth at the minimal mortality cost and maximizes the probability of reaching reproductive size.

Gilliam’s rule of “minimize μ/g” strictly applies only under special conditions, yet as a heuristic tool, it has proved useful in addressing a variety of questions concerning optimal behavioral decisions for organisms under predation risk, including habitat selection, activity level, and life history evolution. Ludwig and Rowe (1990), Rowe and Ludwig (1991), and Abrams and Rowe (1996) have advanced the theory to show how the optimal size at metamorphosis (or size at maturity) is affected by mortality risk in seasonal environments. They show that increased mortality risk in the juvenile stage should favor earlier maturity, which given a fixed growth rate, also means maturing at a smaller size (Abrams and Rowe 1996). Thus, we have the general prediction that increased predation risk in the larval or juvenile stage should lead to earlier metamorphosis and metamorphosis at a smaller body size.

Bobbi Peckarsky and her colleagues tested this prediction by studying the impacts of trout predation on mayfly (Baetis) populations in streams in the Rocky Mountains of the western United States (Peckarsky et al. 1993, 2001, 2002). Mayflies spend most of their lives as larvae, grazing algae from the stream bed and growing through a series of instars until they metamorphose into winged adults, reproduce, and die. Adult mayflies do not feed (in fact, they lack mouthparts) and live for only 2 days. Thus, all the energy needed to develop eggs and reproduce must be gained in the larval stage. Peckarsky and colleagues noted that mayflies from streams containing trout emerged earlier, and at much smaller adult sizes, than conspecifics found in streams without trout (Peckarsky et al. 2001). Thus, their observationsfit the theoretical predictions outlined previously. However, because fish prefer to eat larger prey (as we saw earlier in this chapter), an alternative hypothesis is that size-selective predation was responsible for the difference in the sizes of mayflies hatching from fish-inhabited and fishless streams. To test this hypothesis, the researchers dripped water from tanks containing live trout into fishless streams and compared the size of metamorphosing mayflies in these fish-cue treatment streams with the size of mayflies from fishless streams that received water from tanks without fish (the control). The addition of the fish-cue water (containing chemical signals released by trout) resulted in a 13–21% reduction in adult mayfly size and an estimated 24–35% loss of fecundity (Table 6.1; Peckarsky et al. 2002). Moreover, demographic analysis showed that the reduction in mayfly population growth caused by the nonlethal effects of trout on mayfly size at maturation exceeded the direct effects of consumption by the predator (Table 6.2; McPeek and Peckarsky 1998). Thus, in this remarkably complete series of studies, we see a clear example of how the lethal and sublethal effects of a predator combine to affect the population growth rate of its prey.

##### Activity levels and vigilance

Prey also may reduce their exposure to predators by lowering their activity level (speed and extent of movement), increasing vigilance, and increasing the amount of time they spend in a refuge. A review of the literature shows that such responses are ubiquitous across taxa: almost all species studied exhibited decreased movement, increased vigilance, and/or increased refuge use in response to an increase in the risk of predation (Lima 1998; see Figure 6.10 as an example). However, this reduction in activity may come at the cost of lost feeding time, a lower rate of energy gain, and slowed growth. Thus, there is a growth rate–predation risk trade-off (Abrams 1990; Houston et al. 1993; Werner and Anholt 1993; McPeek 2004). Species differ in how they resolve this trade-off (e.g., Werner and McPeek 1994), and these interspecific differences are thought to be a major mechanism promoting species coexistence. We will explore the question of predator-mediated species coexistence in Chapters 8 and 11.

Figure 6.9 A graphic model illustrating the optimal body size at which to switch habitats as a function of individual growth rates (g) and mortality rates (μ) in each habitat. The optimal size to switch from using habitat 1 to using habitat 2 is the point at which the ratio μ/g in habitat 1 exceeds that ratio in habitat 2 (designated by s’). Predictions of this model may also be applied to the optimal size at which to metamorphose from one life stage to another (e.g., from a tadpole to a frog). After Werner and Gilliam (1984).  

Individuals often feed or travel in groups as a means of reducing predation risk and there is a large literature on how group size and group composition may affect an individual’s risk of mortality (e.g., Lima 1995; Bednekoff 2007; Lehtonen and Jaatinen 2016). In some cases, individual may feed in mixed-species groups where certain species act as “sentinels” for the detection of predators. For example, Martinez et al. (2017) conducted a fascinating study looking at how well two different alarmcalling species (antshrikes, Thamnomanes) served as sentinels in mixed-species flocks (up to 70 bird species) in the Amazonian rainforest. Each mixed flock was led by a single antshrike species. By flying three different species of trained raptors towards the mixed flocks, they discovered that bluish-slate antshrikes (T. schistogynus), which were the sentinel species in flocks occupying forest gaps, produced more alarm calls, and provided more specific information about the size and distance of an approaching predator than did flocks in the dense forest where dusky-throated antshrikes (T. ardesiacus) were the sentinel species. Martinez et al. (2017) suggest that the early successional habitats occupied by bluish-slate antshrikes are more productive, but also riskier. Thus, sentinel species with different levels of vigilance may modulate the peaks and valleys in the landscape of fear.

At the intraspecific level, it is difficult to quantify the impact of reduced prey activity levels on prey population dynamics, especially in comparison to the direct consumptive effects of predators on their prey (Preisser et al. 2005; Creel and Christianson 2008). Moreover, as McPeek (2004) has shown for damselfly larvae, reduced activity in the presence of predators may not directly affect prey feeding rates, but it may affect prey growth and/or survival through physiological processes. Finally, a predator-induced reduction in feeding rate may actually increase prey population growth if this reduction in feeding rate prevents overexploitation of the prey’s resource (Figure 6.11; Abrams 1992; Peacor 2002).

Figure 6.10 Effects of predatory owl threat cues and a variety of other treatments on the activity levels of two gerbil species, Gerbillus allenbyi and G. pyramidum, in the field. Histograms show the ratio of gerbil activity levels in control plots relative to that in treatment plots. Means +1 SE. A ratio of 1.0 indicates no difference in gerbil activity in control and treatment plots; a ratio below 1.0 means that gerbil activity in the treatment plot was less than in the control. Note the strong inhibitory effects of owl flights and owl hunger calls on gerbil activity. After Abramsky et al. (1996). *P < 0.05, **P < 0.001, ***P < 0.0001.  

##### Morphology

Predators have been shown to induce a variety of morphological defenses in their prey populations, as we saw earlier for Paramecium aurelia (Hammill et al. 2010). These inducible defenses include changes in toxicity, color, body shape, shell hardness, and the presence of spines (Figure 6.12), all of which may reduce a prey’s chances of being eaten (see reviews in Harvell 1990; Tollrian and Harvell 1999; Benard 2004). These phenotypically plastic responses can be so dramatic as to cause biologists to mistake the different body forms for different species. A case in point is the crucian carp (Carassius carassius). In lakes without piscivorous predators, crucian carp are found in dense populations of narrow-bodied individuals, but in lakes with piscivores (especially pike, Esox lucius), carp are few in number, and individuals are deep-bodied (Figure 6.13A; Brönmark and Miner 1992). The two carp morphs were originally considered separate species until transplant experiments showed them to be the same (Ekström 1838, cited in Brönmark and Miner 1992; Brönmark and Miner 1992). Ecologists then hypothesized that it was variation in resource levels and growth rates that led to the development of the two morphs. However, a series of field and laboratory studies by Brönmark and colleagues showed that the differences in body form were in fact induced by the presence of predators.

Figure 6.11 An illustration of the potential effect of a predator-induced reduction in consumer (prey) feeding rate on the equilibrium levels of the consumer’s resource. (A) In this example (which assumes logistic growth for the resource), a reduction in the consumer’s feeding rate (from dashed line to solid line in each panel) results in a decrease in equilibrium resource levels at a low feeding rate (top), but an increase in equilibrium resource levels at a high feeding rate (bottom). (B) Average growth rates (±SE) of consumers in an experiment in which the presence of non-lethal predators (caged dragonfly larvae, Anax) reduced the feeding rate of the consumers (large bullfrog tadpoles). In this experiment, the presence of Anax reduced tadpole feeding rates, resulting in higher standing stocks of algae (the tadpole’s resource) and leading to higher growth rates of the tadpoles at medium and high nutrient levels. After Peacor (2002).  

In a field experiment, Brönmark and Miner (1992) introduced young crucian carp from the same population into two ponds. Each pond was divided in half by a plastic curtain, and one half of each pond also received 15–20 pike. After 12 weeks, the carp population had diverged in body shape between the predator and no-predator treatments—in the presence of the pike, carp had significantly deeper bodies (Figure 6.13B). This shift in body form allowed most of the carp to escape being eaten by their pike predators (Figure 6.13C). Importantly, laboratory experiments showed that the observed divergence in body shape could not be accounted for by differences in resource availability and growth rates. Experiments also showed that expression of the deep-bodied form resulted in about a 30% increase in energy expended when swimming compared with the shallow-bodied, more fusiform shape (Brönmark and Miner 1992; Pettersson and Brönmark 1997). Thus, by becoming deeper-bodied, the carp reduced their vulnerability to predation, but paid a significant energetic price. In general, we expect inducible defenses to evolve when predation risk varies temporally or spatially; when prey have the ability to detect predators via reliable cues; and when the defense against predators carries a fitness cost (Brönmark and Hansson 2005).



#### The relative importance of consumptive and non-consumptive effects

The above discussion illustrates the wealth of nonconsumptive effects that predators may have on their prey, but truth be told, we have only scratched the surface of this active and fascinating field. Despite all the excellent research in this area, however, perhaps the most significant questions remain unanswered: “How important are non-consumptive effects relative to consumptive effects in predator– prey interactions?” and “How do we incorporate nonlethal effects into our existing theory of predator– prey interactions” (Abrams 2010)? The challenge in addressing both of these questions comes from the fact that non-consumptive and consumptive effects are often measured in very different units (e.g., growth vs mortality), as well as the fact that these effects may operate on different time scales (e.g., changes in prey behaviors occur much more rapidly than changes in prey densities).

Figure 6.12 Examples of predator-induced defenses in some aquatic organisms. After Peacor (2002).  

Figure 6.13 Differences in body form in the crucian carp (Carassius carassius) are an induced adaptation to reduce mortality by predation. (A) In the presence of predatory fish, crucian carp are very deep-bodied, whereas in the absence of predators, they are narrow-bodied. (B,C) In southern Sweden, Brönmark and Miner divided two small ponds in half with a fish-proof curtain. They introduced juvenile crucian carp and predatory northern pike (Esox lucius) into one half of each pond and juvenile carp only into the other half. (B) After 12 weeks, carp in the presence of pike were much deeper-bodied than carp in the absence of pike. (C) This increase in body depth reduced the carp’s vulnerability to predation. Darker shaded areas denote carp that are too large to be vulnerable to predation. After Brönmark and Miner (1992).  

One common experimental approach to assessing the importance of non-consumptive effects relative to consumptive effects is to compare the impact of a functional predator with that of a non-lethal predator on the density or fitness (measured as fecundity or growth) of the prey population. Non-lethal predator treatments can be created using caged or disabled predators (“risk predators,” such as a spider with non-functional mouthparts; Schmitz 1998) or by introducing a predator cue (e.g., water containing the chemical signal, or kairomone, of a predator). Many such experiments have been performed, and meta-analyses of the results show that the nonlethal effects of predators are often as great as or greater than the lethal effects (Bolnick and Preisser 2005; Preisser et al. 2005). These analyses give us an appreciation of the potential importance of nonconsumptive effects in predator–prey interactions. However, such simple comparisons sweep a number of complications under the rug. For example, the presence of a non-lethal predator in an experiment may result in prey behaviors that reduce feeding rates and growth, resulting in weakened and even starved prey. In nature, however, such weakened prey often fall victim to functional predators. In addition, experimental exposure to non-consumptive predators can lower the subsequent survival of prey and their offspring (McCauley et al. 2011; MacLeod et al. 2018). Thus, the consumptive and non-consumptive effects of predators may interact in nature, and sophisticated experimental designs are needed to accurately estimate their relative importance in predator–prey interactions (e.g., Werner and Peacor 2003). Okuyama and Bolker (2007), Abrams (2008), and Schmitz (2010) provide excellent discussions of these issues, as well as guides to experimental design.



#### Looking ahead

We have seen in this chapter how the dynamic responses of prey to their predators often lead to indirect effects. In a three-link food chain, for example, the presence of the predator may result in increased vigilance and a reduction in feeding rate in the prey, thus increasing the abundance of the prey’s resource. We will discuss this type of indirect interaction (a cascading effect from the predator to the prey’s resource) in much more detail in Chapter 11, where we will examine top-down and bottom-up control in food chains and food webs. Before we get there, however, we need to consider consumer–resource interactions from the point of view of competition (Chapters 7 and 8), as well as the beneficial interactions of mutualism and facilitation (Chapter 9).



#### Summary

1. Predator preference for a prey type can be defined as a difference between the proportion of that prey type in the predator’s diet and the proportion of that prey type in the environment. Predator preference is influenced by:

• the probability that a prey item will be encountered;

• the probability that an encountered prey item will be attacked;

• the probability that an attacked prey item will be captured and eaten.

2. Optimal foraging theory (OFT) is an approach to understanding predator diet choice by asking what set of foraging decisions will maximize a predator’s energy gain. Optimal diet models predict a predator’s diet choices.

3. The standard optimal diet model makes two general predictions and two more specific predictions about predator choice: The two general predictions are:

• foragers should prefer the most profitable prey (i.e., prey that yield the most energy per unit handling time);

• as the overall density of profitable prey decreases, an efficient forager should broaden its diet to include more of the less profitable prey. The two specific predictions of the standard optimal diet model are:

• prey types are either always eaten upon encounter or never eaten upon encounter (the zero-one rule);

• the inclusion of a prey type in the diet depends on its profitability and on the characteristics of prey types of higher profitability.

4. Tests of the optimal diet model support its predictions, but in most cases predators include some prey types in their diets that are not in the predicted optimal set.

5. The MVT predicts when to leave a patch based on how the rate of energy gain in the patch compares with the maximum obtainable overall average energy gain moving across the landscape. When the average productivity (prey availability) 
6. of a patch goes up or the travel time between patches increases, predators will spend a longer time in one patch. One counter-intuitive prediction is that more efficient predators may leave a patch quicker than less efficient predators.
7. Prey may respond to the threat of predation by changing their behaviors, morphologies, physiologies, or life histories. These non-consumptive (or non-lethal) effects of predators act in concert with the direct consumption of prey to influence prey abundance and predator–prey dynamics.
8. In choosing habitats, organisms may face a tradeoff between foraging gain and increased risk of predation. When this is the case, the “optimal” habitat choice depends on the relative costs (mortality risks) and benefits (energy gains) available in each habitat. Thus, for example, increased predation risk in the larval or juvenile stage is predicted to lead to the evolution of earlier metamorphosis and/or a smaller adult body size.
9. Many prey species reduce their activity level in the presence of predators, but these less-active prey may incur a reduction in growth (i.e., a growth rate–predation risk trade-off). Inducible morphological defenses may evolve when predation risk varies temporally or spatially; when prey have the ability to detect predators via reliable cues; and/or when the defense carries a fitness cost.
10. The relative importance of the non-consumptive and consumptive effects can be addressed by performing experiments in which the effect on prey density or fitness of a non-lethal (e.g., caged or disabled) predator or a predator cue is compared with that of a functional predator. However, such experiments do not address the complicated interactions of these effects in nature.